{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Inference"
      ],
      "metadata": {
        "id": "bWMahIVXQx_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries for transformers (Hugging Face library for NLP)\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "YRZMqNQSRgtu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Specify the model to use (pretrained from Hugging Face)\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # You can use other models or your own fine-tuned one model here\n",
        "task = \"text-generation\"\n",
        "prompt = \"Explain how neural networks work in simple terms\"\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "generator = pipeline(task, model=model_name)\n",
        "\n",
        "# Generate text based on the given prompt\n",
        "results = generator(prompt, max_length=100, truncation=True, num_return_sequences=1)\n",
        "\n",
        "# Print the generated output\n",
        "text = results[0].get(\"generated_text\")  # Extract generated text\n",
        "print(f\"Generated text:\\n{text}\")"
      ],
      "metadata": {
        "id": "R1iCaTp9MYyb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Fine-Tuning"
      ],
      "metadata": {
        "id": "uJ5aSbkBQ4OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries for transformers, dataset handling, and logging\n",
        "!pip install -q transformers datasets wandb"
      ],
      "metadata": {
        "id": "fiQCdNtLng5N",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "import wandb\n",
        "\n",
        "# Log in to Weights & Biases (W&B) using Colab secrets\n",
        "wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
        "\n",
        "# Define the model for fine-tuning\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a small custom dataset for training and validation\n",
        "train_data = {\n",
        "    \"text\": [\n",
        "        \"Artificial Intelligence is transforming industries by automating tasks and enabling smarter decision-making.\",\n",
        "        \"Machine learning algorithms learn patterns from data to make predictions and improve over time.\",\n",
        "        \"Neural networks, inspired by the human brain, are the backbone of modern deep learning systems.\",\n",
        "        \"Self-driving cars rely on AI to navigate roads, detect obstacles, and make real-time decisions.\",\n",
        "    ]\n",
        "}\n",
        "\n",
        "val_data = {\n",
        "    \"text\": [\n",
        "        \"AI has the potential to solve some of the world's most pressing challenges, from healthcare to climate change.\",\n",
        "        \"The collaboration between humans and AI will define the next era of technological innovation.\"\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "kZQtIwDfllT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# For models like GPT-2 that don't have a pad token, assign the eos token as pad_token.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Convert dataset into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "val_dataset = Dataset.from_dict(val_data)\n",
        "\n",
        "# Define tokenization function with truncation and padding\n",
        "max_length = 64  # Maximum sequence length\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "# Apply tokenization to datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Add labels by copying input_ids\n",
        "# The labels are simply the input_ids because the model learns to predict the next token.\n",
        "tokenized_train = tokenized_train.map(lambda examples: {\"labels\": examples[\"input_ids\"]})\n",
        "tokenized_val = tokenized_val.map(lambda examples: {\"labels\": examples[\"input_ids\"]})"
      ],
      "metadata": {
        "id": "dgxzv_JVl5me",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"AI-Tuned-DeepSeek-R1-Distill-Qwen-1.5B\",  # Directory to save model checkpoints\n",
        "    learning_rate=0.0001,  # Learning rate for fine-tuning\n",
        "    per_device_train_batch_size=2,  # Reduce batch size for low memory usage\n",
        "    per_device_eval_batch_size=2,\n",
        "    eval_strategy=\"steps\",  # Evaluate periodically during training\n",
        "    num_train_epochs=1,  # Number of training epochs\n",
        "    eval_steps=2,  # Perform evaluation every 2 steps\n",
        "    logging_steps=2,  # Log training details every 2 steps\n",
        "    save_steps=2,  # Save model checkpoints every 2 steps\n",
        "    load_best_model_at_end=True,  # Load best model after training\n",
        "    report_to=[\"wandb\"],  # Log metrics to Weights & Biases\n",
        "    push_to_hub=True,  # Upload model to Hugging Face Hub\n",
        ")\n",
        "\n",
        "# Initialize the Trainer class\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model (this will log training metrics to W&B and evaluate on the validation set)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "44-ykrRBRQ1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}