{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnPB2bnjTGFo"
      },
      "source": [
        "# 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NdwIKvA2le6"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install -q transformers requests torch huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BAd-W5iALgLN"
      },
      "outputs": [],
      "source": [
        "# Import userdata module from Google Colab\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Hugging Face API token securely from Colab Secrets\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpcy4QV-2wCb"
      },
      "source": [
        "# 1. Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtwDut1RTKpo"
      },
      "source": [
        "## 1.1 Local Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQnDzmbX2xaJ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize text-generation pipeline locally\n",
        "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
        "\n",
        "# Generate text from a prompt\n",
        "results = generator(\"The future of AI is\", truncation=True, max_length=30)\n",
        "\n",
        "# Print generated text\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMsB_1uW3N2L"
      },
      "source": [
        "## 1.2 API with requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdgd_EmK3Vxr"
      },
      "outputs": [],
      "source": [
        "# Import requests library\n",
        "import requests\n",
        "\n",
        "# Set API authorization header\n",
        "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "# API endpoint for text generation\n",
        "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "# Define input and generation parameters\n",
        "payload = {\n",
        "    \"inputs\": \"The future of AI is\",\n",
        "    \"parameters\": {\n",
        "        \"max_length\": 30,   # Maximum length of generated text (in tokens)\n",
        "        \"temperature\": 0.7, # Controls randomness; higher values increase creativity\n",
        "        \"top_p\": 0.9        # Controls diversity via nucleus sampling (probability threshold)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Send request to Hugging Face API\n",
        "response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "# Output response from API\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_9giZUATZz_"
      },
      "source": [
        "## 1.3 API with huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rJwG6nfThtH"
      },
      "outputs": [],
      "source": [
        "# Import the InferenceClient from Hugging Face Hub\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Initialize the Hugging Face Inference Client with provider and API key\n",
        "client = InferenceClient(\n",
        "    provider=\"hf-inference\",  # Specifies Hugging Face inference provider\n",
        "    api_key=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Define the conversation history with the user's message\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",  # Role of the sender (user input)\n",
        "        \"content\": \"The future of AI is\"  # User's input text\n",
        "    }\n",
        "]\n",
        "\n",
        "# Generate a chat completion using the specified model and parameters\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct\",  # Model repository ID\n",
        "    messages=messages,  # Message history for conversation\n",
        "    max_tokens=30,  # Limit the response to 30 tokens\n",
        "    temperature=0.7,  # Controls randomness; higher values increase creativity\n",
        "    top_p=0.9,  # Nucleus sampling threshold; controls response diversity\n",
        ")\n",
        "\n",
        "# Print the generated response from the model\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWXmFQa4-EFs"
      },
      "source": [
        "# 2. Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfptcQCTZODp"
      },
      "source": [
        "## 2.1 Local Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUMpuXnd-I8r"
      },
      "outputs": [],
      "source": [
        "# Import pipeline from the transformers library\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize a text classification pipeline with a pre-trained sentiment analysis model\n",
        "classifier = pipeline(\"text-classification\",\n",
        "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Classify example texts and print the results\n",
        "print(f\"Example 1: {classifier('I love AI!')}\")  # Expected output: Positive sentiment\n",
        "print(f\"Example 2: {classifier('I hate AI!')}\")  # Expected output: Negative sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dnos7Z--JFi"
      },
      "source": [
        "## 2.2 API with requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmdLGc1Z-Vs_"
      },
      "outputs": [],
      "source": [
        "# Import the requests library for making HTTP requests\n",
        "import requests\n",
        "\n",
        "# Define the authorization headers with the Hugging Face API token\n",
        "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "# Define the Hugging Face API URL for sentiment analysis\n",
        "API_URL = \"https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# Prepare the input text for sentiment classification\n",
        "payload = {\"inputs\": \"I love AI!\"}\n",
        "\n",
        "# Send a POST request to the Hugging Face API with the input text\n",
        "response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "# Print the API response containing the sentiment prediction\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCDGb0wxZXDe"
      },
      "source": [
        "## 2.3 API with huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYLjjWvxZa7_"
      },
      "outputs": [],
      "source": [
        "# Import the InferenceClient from Hugging Face Hub\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Initialize the Hugging Face Inference Client with provider and API key\n",
        "client = InferenceClient(\n",
        "    provider=\"hf-inference\",  # Specifies Hugging Face inference provider\n",
        "    api_key=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Perform text classification using the specified model\n",
        "result = client.text_classification(\n",
        "    text=\"I love AI!\",  # Input text\n",
        "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",  # Pre-trained sentiment model\n",
        ")\n",
        "\n",
        "# Print the classification result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da4K8tR-V4a"
      },
      "source": [
        "# 3. Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMxbWRZ-Zfqu"
      },
      "source": [
        "## 3.1 Local Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IiBoZFJ-Y0p"
      },
      "outputs": [],
      "source": [
        "# Import pipeline from transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize image-classification pipeline\n",
        "image_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
        "\n",
        "# Image file for classification\n",
        "image_url = \"cat_sample.png\"\n",
        "\n",
        "# Classify image and print results\n",
        "print(image_classifier(image_url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tpsAGPCZnYf"
      },
      "source": [
        "## 3.2 API with requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpDo2Rn1DSVg"
      },
      "outputs": [],
      "source": [
        "# Import requests library\n",
        "import requests\n",
        "\n",
        "# Set API authorization header\n",
        "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "# API endpoint for image classification\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/vit-base-patch16-224\"\n",
        "\n",
        "# Load image file in binary mode\n",
        "image_url = \"cat_sample.png\"\n",
        "with open(image_url, \"rb\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Send image data to Hugging Face API\n",
        "response = requests.post(API_URL, headers=headers, data=data)\n",
        "\n",
        "# Output image classification results\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuiGfk5a-Y3W"
      },
      "source": [
        "## 3.3 API with huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7J_eMkUb1_Z"
      },
      "outputs": [],
      "source": [
        "# Import the InferenceClient from Hugging Face Hub\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Initialize the Hugging Face Inference Client with provider and API key\n",
        "client = InferenceClient(\n",
        "    provider=\"hf-inference\",  # Specifies Hugging Face inference provider\n",
        "    api_key=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Read the image file in binary mode\n",
        "with open(\"cat_sample.png\", \"rb\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "# Perform image classification using the specified model\n",
        "result = client.image_classification(\n",
        "    image=data,\n",
        "    model=\"google/vit-base-patch16-224\",\n",
        ")\n",
        "\n",
        "# Print the classification result\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}